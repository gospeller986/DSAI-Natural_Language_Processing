# -*- coding: utf-8 -*-
"""DSAI_mt_sample(dataset).ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1WBlbVLqehUhkd2lp0V2HmdL_ZG8zwQvG
"""

import spacy
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
import string
import re

from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.decomposition import PCA
from sklearn.linear_model import LogisticRegression
from sklearn.model_selection import train_test_split

from sklearn.metrics import confusion_matrix
from sklearn.metrics import classification_report
from sklearn.manifold import TSNE

from nltk.tokenize import word_tokenize
from nltk.tokenize import sent_tokenize
from nltk.stem import WordNetLemmatizer

df = pd.read_csv('mtsamples.csv')

df.head()

df['medical_specialty'].count()

df = df[df['transcription'].notna()]
df = df[df['keywords'].notna()]

df.count()

diseases  = df.groupby(df['medical_specialty'])
i = 1

for dName,dCategory in diseases:
    
     print(' '+dName + ' : '+ str(len(dCategory)) )
     i = i+1



selected_df = diseases.filter(lambda x:x.shape[0] > 75)
selected_df.head()

plt.figure(figsize=(10,10))
sns.countplot(y='medical_specialty', data = selected_df )
plt.show()

"""**WE HAVE OBSERVED THAT WE ARE DEALING WITH A HIGHLY IMBALANCED DATASET**"""

data = selected_df[['transcription', 'medical_specialty']]

pip install https://s3-us-west-2.amazonaws.com/ai2-s2-scispacy/releases/v0.4.0/en_ner_bc5cdr_md-0.4.0.tar.gz

pip install spacy>=3.0

pip install scispacy

import scispacy
import spacy
nlp = spacy.load("en_ner_bc5cdr_md")

nlp

def func(sample):
  
  #name entity recognition 
  doc = nlp(sample)
  vec = []
  for ent in doc.ents:
    vec.append(ent.text)
  
  return vec

feature = []

data.count()



rev_df = pd.DataFrame(data)

rev_df = rev_df.reset_index(drop=True)
rev_df.head()

for i in range(0,3331):
  sample = rev_df['transcription'][i]
  feature.append(func(sample))



feature

rev_df['key_feature_extracted'] = feature
rev_df

import nltk
nltk.download('punkt')
from nltk.corpus import stopwords
nltk.download('stopwords')
from nltk.tokenize import word_tokenize

nltk.download('wordnet')
nltk.download('omw-1.4')

from nltk.stem import WordNetLemmatizer

lemmatizer = WordNetLemmatizer()

def pipeline(sample):
  tokens = word_tokenize(sample)
  # tokens_without_sw = [word for word in tokens if not word in stopwords.words()]
  tokens_lemma = [] 
  for j in tokens:
     tokens_lemma.append(lemmatizer.lemmatize(j))
  x = ''
  for i in tokens_lemma:
   x = x + " " +i 

  return x

revised = []

for i in range(0,3331):
  sample = rev_df['transcription'][i]
  revised.append(pipeline(sample))

len(revised)

rev_df['desc_after_lemma'] = revised
rev_df.head()

def sw_removal(text): 
    text = text.translate(str.maketrans('', '', string.punctuation))
    text1 = ''.join([w for w in text if not w.isdigit()]) 
    REPLACE_BY_SPACE_RE = re.compile('[/(){}\[\]\|@,;]')
    #BAD_SYMBOLS_RE = re.compile('[^0-9a-z #+_]')
    
    text2 = text1.lower()
    text2 = REPLACE_BY_SPACE_RE.sub('', text2) # replace REPLACE_BY_SPACE_RE symbols by space in text
    #text2 = BAD_SYMBOLS_RE.sub('', text2)
    return text2

rev_df['desc_after_lemma_and_sw_removal'] = rev_df['desc_after_lemma'].apply(sw_removal)

rev_df.head()

vectorizer = TfidfVectorizer(analyzer='word', stop_words='english',ngram_range=(1,3), max_df=0.75, use_idf=True, smooth_idf=True, max_features=1000)
tfIdfMat  = vectorizer.fit_transform(rev_df['desc_after_lemma_and_sw_removal'].tolist() )
feature_names = sorted(vectorizer.get_feature_names_out())

gc.collect()
pca = PCA(n_components=0.95)
tfIdfMat_reduced = pca.fit_transform(tfIdfMat.toarray())
labels = data['medical_specialty'].tolist()
category_list = data.medical_specialty.unique()
X_train, X_test, y_train, y_test = train_test_split(tfIdfMat_reduced, labels, stratify=labels,random_state=1)

print('Train_Set_Size:'+str(X_train.shape))
print('Test_Set_Size:'+str(X_test.shape))

clf = LogisticRegression(penalty= 'elasticnet', solver= 'saga', l1_ratio=0.5, random_state=1).fit(X_train, y_train)
y_test_pred= clf.predict(X_test)

from sklearn.metrics import confusion_matrix
labels = category_list
cm = confusion_matrix(y_test, y_test_pred )

fig = plt.figure(figsize=(20,20))
ax= fig.add_subplot(1,1,1)
sns.heatmap(cm, annot=True, cmap="Greens",ax = ax,fmt='g'); #annot=True to annotate cells

# labels, title and ticks
ax.set_xlabel('Predicted labels');ax.set_ylabel('True labels'); 
ax.set_title('Confusion Matrix'); 
ax.xaxis.set_ticklabels(labels); ax.yaxis.set_ticklabels(labels);
plt.setp(ax.get_yticklabels(), rotation=30, horizontalalignment='right')
plt.setp(ax.get_xticklabels(), rotation=30, horizontalalignment='right')     
plt.show()

print(classification_report(y_test,y_test_pred,labels=category_list))





